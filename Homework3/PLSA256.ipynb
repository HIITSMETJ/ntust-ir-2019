{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T07:27:52.478115Z",
     "start_time": "2019-11-15T07:27:51.620747Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from numba import jit\n",
    "from scipy.sparse import coo_matrix\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T07:27:53.139849Z",
     "start_time": "2019-11-15T07:27:52.480457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of collection: 18461\n"
     ]
    }
   ],
   "source": [
    "text_file = open('./Collection.txt', \"r\")\n",
    "clcs = text_file.read().splitlines()\n",
    "clc_list = []\n",
    "for clc in clcs:\n",
    "    content = clc.split()\n",
    "    content = [x for x in content]\n",
    "    cnt_str = ' '.join(content)\n",
    "    clc_list.append(cnt_str)\n",
    "text_file.close()\n",
    "\n",
    "print('size of collection:',len(clc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T07:27:53.573993Z",
     "start_time": "2019-11-15T07:27:53.142146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of document: 2265\n"
     ]
    }
   ],
   "source": [
    "text_file = open('./doc_list.txt', \"r\")\n",
    "docs = text_file.read().splitlines()\n",
    "doc_list = []\n",
    "for doc in docs:\n",
    "    f = open('./Document/' + doc)\n",
    "    content = f.read().split()[5:]\n",
    "    content = [x for x in content if x != '-1']\n",
    "    cnt_str = ' '.join(content)\n",
    "    doc_list.append(cnt_str)\n",
    "text_file.close()\n",
    "\n",
    "print('size of document:', len(doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T07:27:53.586185Z",
     "start_time": "2019-11-15T07:27:53.575674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of query: 32\n"
     ]
    }
   ],
   "source": [
    "text_file = open('./query_list.txt', \"r\")\n",
    "queries = text_file.read().splitlines()\n",
    "qry_list = []\n",
    "for qry in queries:\n",
    "    f = open('./Query/' + qry)\n",
    "    content = f.read().split()\n",
    "    content = [x for x in content if x != '-1']\n",
    "    qry_list.append(content)\n",
    "text_file.close()\n",
    "\n",
    "print('size of query:', len(qry_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T07:27:53.717439Z",
     "start_time": "2019-11-15T07:27:53.587883Z"
    }
   },
   "outputs": [],
   "source": [
    "text_file = open('./BGLM.txt', \"r\")\n",
    "BGLM = text_file.read().splitlines()\n",
    "idf={}\n",
    "for line in BGLM:\n",
    "    (word,value)=line.split()\n",
    "    idf[word]=np.exp(float(value))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T07:27:57.898915Z",
     "start_time": "2019-11-15T07:27:53.719178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 35028\n"
     ]
    }
   ],
   "source": [
    "# build clc_tf, vocabulary\n",
    "vectorizer = CountVectorizer(token_pattern='[0-9]+', min_df = 1)\n",
    "clc_tf = vectorizer.fit_transform(clc_list+doc_list).tocoo()\n",
    "vocabulary = vectorizer.vocabulary_  # Mapping of {word -> col of doc_term}\n",
    "print('size of vocabulary:', len(vocabulary))\n",
    "\n",
    "# build doc_tf\n",
    "doc_tf = vectorizer.transform(doc_list).tocoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Pwt,Ptd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T07:27:57.904825Z",
     "start_time": "2019-11-15T07:27:57.900620Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_Pwt(T, V):\n",
    "    Pwt=np.random.rand(T, V)\n",
    "    Pwt /=  Pwt.sum(axis=1,keepdims=True)\n",
    "    return Pwt\n",
    "\n",
    "def init_Ptd(D, T):\n",
    "    Ptd=np.random.rand(D, T)\n",
    "    Ptd /=  Ptd.sum(axis=1,keepdims=True)\n",
    "    return Ptd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T07:27:57.937246Z",
     "start_time": "2019-11-15T07:27:57.906523Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def E_step(tf_data,tf_row,tf_col, Pwt, Ptd, T):\n",
    "    nnz=len(tf_data)\n",
    "    Ptwd = np.zeros((T, nnz))    \n",
    "    for ij in range(nnz):\n",
    "        j, i = tf_row[ij], tf_col[ij]\n",
    "        Ptwd_numerator = np.zeros(T)\n",
    "        Ptwd_denominator = 0\n",
    "        for k in range(T):\n",
    "            Ptwd_numerator[k] = Pwt[k][i] * Ptd[j][k]\n",
    "            Ptwd_denominator += Pwt[k][i] * Ptd[j][k]\n",
    "        for k in range(T):\n",
    "            if Ptwd_denominator!=0:\n",
    "                Ptwd[k][ij] = Ptwd_numerator[k] / Ptwd_denominator\n",
    "    return Ptwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T07:27:57.945019Z",
     "start_time": "2019-11-15T07:27:57.938956Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def M_step(tf_data,tf_row,tf_col, Ptwd, clc_len, V, C, T):\n",
    "\n",
    "    Ptd = np.zeros((C, T))\n",
    "    Pwt = np.zeros((T, V))\n",
    "    Pwt_denominator = np.zeros(T)\n",
    "    for ij in range(len(tf_data)):\n",
    "        j, i = tf_row[ij], tf_col[ij]\n",
    "        for k in range(T):            \n",
    "            Pwt[k][i] += tf_data[ij]*Ptwd[k][ij]\n",
    "            Pwt_denominator[k] += tf_data[ij]*Ptwd[k][ij]\n",
    "            Ptd[j][k] += tf_data[ij]*Ptwd[k][ij]\n",
    "    for k in range(T): \n",
    "        for i in range(V):\n",
    "            Pwt[k][i] = Pwt[k][i] / Pwt_denominator[k] \n",
    "    for k in range(T):\n",
    "        for j in range(C):\n",
    "            Ptd[j][k] = Ptd[j][k] / clc_len[j]\n",
    "    return Pwt, Ptd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M step (fold-in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T07:27:57.950840Z",
     "start_time": "2019-11-15T07:27:57.946736Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def M_step_fold_in(tf_data,tf_row,tf_col, Ptwd, doc_len, D, T):\n",
    "\n",
    "    Ptd_fdn = np.zeros((D, T))\n",
    "    for ij in range(len(tf_data)):\n",
    "        j, i = tf_row[ij], tf_col[ij]\n",
    "        for k in range(T):\n",
    "            Ptd_fdn[j][k] += tf_data[ij]*Ptwd[k][ij]  \n",
    "            \n",
    "    for k in range(T): \n",
    "        for j in range(D):\n",
    "            Ptd_fdn[j][k] = Ptd_fdn[j][k] / doc_len[j]  \n",
    "    return Ptd_fdn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T07:27:57.956122Z",
     "start_time": "2019-11-15T07:27:57.952627Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def likelihood(tf_data, tf_row, tf_col, Pwt, Ptd, T):\n",
    "    likelihood=0\n",
    "    for ij in range(len(tf_data)):\n",
    "        j,i = tf_row[ij], tf_col[ij]\n",
    "        sumation=0\n",
    "        for k in range(T):\n",
    "            sumation+=Pwt[k][i]*Ptd[j][k]\n",
    "        likelihood+=tf_data[ij]*np.log(sumation)\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T07:27:57.962040Z",
     "start_time": "2019-11-15T07:27:57.957895Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def similarity(alpha, beta, Pwd, PLSA):\n",
    "\n",
    "    sim = np.zeros((Q, D))\n",
    "    for q in range(Q):\n",
    "        for word in (qry_list[q]):\n",
    "                if word in vocabulary:\n",
    "                    i = vocabulary[word]\n",
    "                    A = alpha * Pwd[:,i]\n",
    "                    B = beta * PLSA[:,i]\n",
    "                    C = (1 - alpha - beta) * idf[word]\n",
    "                    sim[q,:] += np.log(A + B + C)\n",
    "        if q % 4 == 3:\n",
    "            print(\"query:\",q+1)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T07:28:33.772837Z",
     "start_time": "2019-11-15T07:28:30.634147Z"
    }
   },
   "outputs": [],
   "source": [
    "C = len(clc_list+doc_list)\n",
    "V = len(vocabulary)\n",
    "T = 256\n",
    "Pwt = init_Pwt(T, V)\n",
    "Ptd = init_Ptd(C, T)\n",
    "# Pwt = np.load(\"Pwt.npy\")\n",
    "# Ptd = np.load(\"Ptd.npy\")\n",
    "clc_data = clc_tf.data\n",
    "clc_row = clc_tf.row\n",
    "clc_col = clc_tf.col\n",
    "clc_len = clc_tf.toarray().sum(axis=1).reshape((C,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T09:18:34.014069Z",
     "start_time": "2019-11-15T07:28:34.034803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 20 \tlikelihood: -39473196.938522205\n",
      "iter: 40 \tlikelihood: -38491133.57485001\n",
      "iter: 60 \tlikelihood: -38251146.91385805\n",
      "iter: 80 \tlikelihood: -38140396.86830778\n",
      "iter: 100 \tlikelihood: -38075253.11421587\n",
      "iter: 120 \tlikelihood: -38031666.210489586\n",
      "iter: 140 \tlikelihood: -38000660.77803813\n",
      "iter: 160 \tlikelihood: -37976712.224048235\n",
      "iter: 180 \tlikelihood: -37957182.664669566\n",
      "iter: 200 \tlikelihood: -37941024.915237986\n"
     ]
    }
   ],
   "source": [
    "# train_iter = 200\n",
    "print(\"iter:\", 0, \"\\tlikelihood:\", likelihood(clc_data,clc_row,clc_col,Pwt,Ptd,T))\n",
    "for count in range(200):\n",
    "    Ptwd = E_step(clc_data,clc_row,clc_col, Pwt, Ptd, T)\n",
    "    Pwt, Ptd = M_step(clc_data,clc_row,clc_col, Ptwd, clc_len, V, C, T)\n",
    "    if count%20==19:\n",
    "        np.save('Pwt', Pwt)\n",
    "        np.save('Ptd', Ptd)\n",
    "        print(\"iter:\", count+1, \"\\tlikelihood:\", likelihood(clc_data,clc_row,clc_col,Pwt,Ptd,T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fold-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T09:48:05.312321Z",
     "start_time": "2019-11-15T09:48:04.976421Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D = len(doc_list)\n",
    "Pwt = np.load(\"Pwt.npy\")\n",
    "Ptd_fdn = init_Ptd(D, T)\n",
    "# Ptd_fdn = np.load(\"Ptd_fdn.npy\")\n",
    "doc_data = doc_tf.data\n",
    "doc_row = doc_tf.row\n",
    "doc_col = doc_tf.col\n",
    "doc_len = doc_tf.toarray().sum(axis=1).reshape((D,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T09:52:16.957839Z",
     "start_time": "2019-11-15T09:48:08.054123Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 20 \tlikelihood: -2457532.0504565504\n",
      "iter: 40 \tlikelihood: -2457284.071879985\n",
      "iter: 60 \tlikelihood: -2457255.814712747\n",
      "iter: 80 \tlikelihood: -2457248.4411781672\n",
      "iter: 100 \tlikelihood: -2457245.674834265\n",
      "iter: 120 \tlikelihood: -2457244.4017677656\n",
      "iter: 140 \tlikelihood: -2457243.735669609\n",
      "iter: 160 \tlikelihood: -2457243.3507889854\n",
      "iter: 180 \tlikelihood: -2457243.1089782664\n",
      "iter: 200 \tlikelihood: -2457242.947904937\n"
     ]
    }
   ],
   "source": [
    "# fold_in_iter = 200\n",
    "for count in range(200):\n",
    "    Ptwd = E_step(doc_data,doc_row,doc_col, Pwt, Ptd_fdn, T)\n",
    "    Ptd_fdn = M_step_fold_in(doc_data,doc_row,doc_col, Ptwd, doc_len, D, T)\n",
    "    if count%20==19:\n",
    "        np.save('Ptd_fdn', Ptd_fdn)\n",
    "        print(\"iter:\", count+1, \"\\tlikelihood:\", likelihood(doc_data,doc_row,doc_col,Pwt,Ptd_fdn,T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T09:53:37.292318Z",
     "start_time": "2019-11-15T09:53:29.903371Z"
    }
   },
   "outputs": [],
   "source": [
    "Q = len(qry_list)\n",
    "D = len(doc_list)\n",
    "# Ptd_fdn = np.load(\"Ptd_fdn.npy\")\n",
    "# Pwt = np.load(\"Pwt.npy\")\n",
    "Pwd = doc_tf.toarray() / doc_len.reshape((D,1))\n",
    "PLSA = np.dot(Ptd_fdn, Pwt)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T06:43:02.930586Z",
     "start_time": "2019-11-10T06:43:02.926109Z"
    }
   },
   "source": [
    "train     fold_in     alpha, beta     score\n",
    "+0        +0          (0.1, 0.7)      0.47828\n",
    "+0        +0          (0.1, 0.4)      0.49409\n",
    "+0        +0          (0.1, 0.1)      0.49749\n",
    "+50       +50         (0.1, 0)        0.49382\n",
    "+50       +50         (0.12, 0)       0.50087\n",
    "+50       +50         (0.13, 0.3)     0.50140\n",
    "+0        +0          (0.14, 0)       0.50039\n",
    "+0        +0          (0.1, 0.05)     0.49415\n",
    "+0        +0          (0.05, 0.05)    0.49264\n",
    "+0        +0          (0.15, 0.05)    0.49442\n",
    "+50       +50         (0.14, 0)       0.50039\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T09:53:51.580893Z",
     "start_time": "2019-11-15T09:53:50.091021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: 4\n",
      "query: 8\n",
      "query: 12\n",
      "query: 16\n",
      "query: 20\n",
      "query: 24\n",
      "query: 28\n",
      "query: 32\n"
     ]
    }
   ],
   "source": [
    "alpha, beta = (0.1, 0.1)\n",
    "sim = similarity(alpha, beta, Pwd, PLSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T09:53:51.653101Z",
     "start_time": "2019-11-15T09:53:51.582708Z"
    }
   },
   "outputs": [],
   "source": [
    "fname = \"./256_\"+str(alpha)+\"_\"+str(beta)+\".txt\"\n",
    "f = open(fname, 'w')\n",
    "f.write(\"Query,RetrievedDocuments\\n\")  \n",
    "\n",
    "for q in range(len(qry_list)):\n",
    "    f.write(queries[q] + \",\")        \n",
    "    rank = np.argsort(-sim[q])\n",
    "    for j in rank:\n",
    "        f.write(docs[j]+\" \")\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T04:46:16.215856Z",
     "start_time": "2019-11-10T04:46:16.204076Z"
    }
   },
   "source": [
    "text_file = open(\"./result_\"+str(alpha)+\"_\"+str(beta)+\".txt\", \"r\")\n",
    "result = text_file.read().splitlines()\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T04:46:16.707009Z",
     "start_time": "2019-11-10T04:46:16.699371Z"
    }
   },
   "source": [
    "text_file = open('./result_49749.txt', \"r\")\n",
    "result_last = text_file.read().splitlines()\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T04:46:17.959094Z",
     "start_time": "2019-11-10T04:46:17.955062Z"
    }
   },
   "source": [
    "result==result_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TJ]",
   "language": "python",
   "name": "conda-env-TJ-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
