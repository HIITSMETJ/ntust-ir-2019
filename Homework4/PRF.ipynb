{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:56:20.589885Z",
     "start_time": "2019-11-18T09:56:19.893774Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from numba import jit\n",
    "from scipy.sparse import coo_matrix\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:56:21.273355Z",
     "start_time": "2019-11-18T09:56:20.591763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of collection: 18461\n"
     ]
    }
   ],
   "source": [
    "text_file = open('./Collection.txt', \"r\")\n",
    "clcs = text_file.read().splitlines()\n",
    "clc_list = []\n",
    "for clc in clcs:\n",
    "    content = clc.split()\n",
    "    content = [x for x in content]\n",
    "    cnt_str = ' '.join(content)\n",
    "    clc_list.append(cnt_str)\n",
    "text_file.close()\n",
    "\n",
    "print('size of collection:',len(clc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:56:21.686702Z",
     "start_time": "2019-11-18T09:56:21.275213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of document: 2265\n"
     ]
    }
   ],
   "source": [
    "text_file = open('./doc_list.txt', \"r\")\n",
    "docs = text_file.read().splitlines()\n",
    "doc_list = []\n",
    "for doc in docs:\n",
    "    f = open('./Document/' + doc)\n",
    "    content = f.read().split()[5:]\n",
    "    content = [x for x in content if x != '-1']\n",
    "    cnt_str = ' '.join(content)\n",
    "    doc_list.append(cnt_str)\n",
    "text_file.close()\n",
    "\n",
    "print('size of document:', len(doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:56:21.858465Z",
     "start_time": "2019-11-18T09:56:21.689072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of query: 800\n"
     ]
    }
   ],
   "source": [
    "text_file = open('./query_list.txt', \"r\")\n",
    "queries = text_file.read().splitlines()\n",
    "qry_list = []\n",
    "for qry in queries:\n",
    "    f = open('./Query/' + qry)\n",
    "    content = f.read().split()\n",
    "    content = [x for x in content if x != '-1']\n",
    "    qry_list.append(content)\n",
    "text_file.close()\n",
    "\n",
    "print('size of query:', len(qry_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:56:22.001346Z",
     "start_time": "2019-11-18T09:56:21.860774Z"
    }
   },
   "outputs": [],
   "source": [
    "text_file = open('./BGLM.txt', \"r\")\n",
    "BGLM = text_file.read().splitlines()\n",
    "idf={}\n",
    "for line in BGLM:\n",
    "    (word,value)=line.split()\n",
    "    idf[word]=np.exp(float(value))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:56:22.699069Z",
     "start_time": "2019-11-18T09:56:22.003156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of Rq_list: (800, 50)\n"
     ]
    }
   ],
   "source": [
    "text_file = open('./Rq100.txt', \"r\")\n",
    "Rqs = text_file.read().splitlines()\n",
    "Rq_list = []\n",
    "for Rq in Rqs:\n",
    "    content = Rq.split()\n",
    "    content = [doc_list[int(j)].split() for j in content]\n",
    "    Rq_list.append(content)\n",
    "text_file.close()\n",
    "\n",
    "print('size of Rq_list:',(len(Rq_list),len(Rq_list[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:56:26.797169Z",
     "start_time": "2019-11-18T09:56:22.700969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 35028\n"
     ]
    }
   ],
   "source": [
    "# build clc_tf, vocabulary\n",
    "vectorizer = CountVectorizer(token_pattern='[0-9]+', min_df = 1)\n",
    "clc_tf = vectorizer.fit_transform(clc_list+doc_list).tocoo()\n",
    "vocabulary = vectorizer.vocabulary_  # Mapping of {word -> col of doc_term}\n",
    "print('size of vocabulary:', len(vocabulary))\n",
    "\n",
    "# build doc_tf\n",
    "doc_tf = vectorizer.transform(doc_list).tocoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:05:46.658040Z",
     "start_time": "2019-11-17T07:05:46.648686Z"
    }
   },
   "source": [
    "# PRF_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T09:56:26.830432Z",
     "start_time": "2019-11-18T09:56:26.799044Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def PRF_similarity(Pwd, PLSA, Rq_list, rev_num, q_ratio, Rq_ratio):\n",
    "    alpha = 0.1\n",
    "    beta = 0.52\n",
    "    PRF_sim = np.zeros((Q, D))\n",
    "    for q in range(Q):\n",
    "        # original query part\n",
    "        qry_sim = np.zeros(D)\n",
    "        q_len = len(qry_list[q])\n",
    "        for word in qry_list[q]:\n",
    "                if word in vocabulary:\n",
    "                    i = vocabulary[word]\n",
    "                    A = alpha * Pwd[:,i]\n",
    "                    B = beta * PLSA[:,i]\n",
    "                    C = (1 - alpha - beta) * idf[word]\n",
    "                    qry_sim[:] += np.log(A + B + C)\n",
    "        # pseudo relvancr feedback part            \n",
    "        Rq_sim = np.zeros(D)\n",
    "        Rq_len = 0\n",
    "        for r in range(rev_num):\n",
    "            Rq_len += len(Rq_list[q][r])\n",
    "            for word in Rq_list[q][r]:\n",
    "                    i = vocabulary[word]\n",
    "                    A = alpha * Pwd[:,i]\n",
    "                    B = beta * PLSA[:,i]\n",
    "                    C = (1 - alpha - beta) * idf[word]\n",
    "                    Rq_sim[:] += np.log(A + B + C)\n",
    "                    \n",
    "        PRF_sim[q,:] = q_ratio/q_len*qry_sim[:] + Rq_ratio/Rq_len*Rq_sim[:]\n",
    "        if q % 100 == 99:\n",
    "            print(\"query:\",q+1)\n",
    "    return PRF_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRF result"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "T      (rev_num, q_ratio, Rq_ratio)     score\n",
    "128    (10, 0.8, 0.2)                   0.63774\n",
    "128    (10, 0.6, 0.4)                   0.65603\n",
    "128    (10, 0.5, 0.5)                   0.66177\n",
    "128    (10, 0.4, 0.6)                   0.66685\n",
    "128    (10, 0.3, 0.7)                   0.66902\n",
    "128    (15, 0.3, 0.7)                   0.67470\n",
    "128    (20, 0.3, 0.7)                   0.67670\n",
    "128    (20, 0.4, 0.6)                   0.67460\n",
    "128    (20, 0.2, 0.8)                   0.67854\n",
    "100    (20, 0.2, 0.8)                   0.68112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T10:06:00.603941Z",
     "start_time": "2019-11-18T09:56:26.832261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: 300\n",
      "query: 400\n",
      "query: 500\n",
      "query: 600\n",
      "query: 700\n",
      "query: 800\n"
     ]
    }
   ],
   "source": [
    "T = 100\n",
    "D = len(doc_list)\n",
    "Q = len(qry_list)\n",
    "Pwd = np.load(\"Pwd\"+str(T)+\".npy\")\n",
    "PLSA = np.load(\"PLSA\"+str(T)+\".npy\")\n",
    "(rev_num, q_ratio, Rq_ratio) = (20, 0.2, 0.8)\n",
    "PRF_sim = PRF_similarity(Pwd, PLSA, Rq_list, rev_num, q_ratio, Rq_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T10:06:00.780614Z",
     "start_time": "2019-11-18T10:06:00.605631Z"
    }
   },
   "outputs": [],
   "source": [
    "fname = \"./\"+str(T)+'_'+str(rev_num)+'_'+str(q_ratio)+'_'+str(Rq_ratio)+\".txt\"\n",
    "f = open(fname, 'w')\n",
    "f.write(\"Query,RetrievedDocuments\\n\")  \n",
    "\n",
    "for q in range(len(qry_list)):\n",
    "    f.write(queries[q] + \",\")        \n",
    "    rank = np.argsort(-PRF_sim[q])\n",
    "    for j in rank[:50]:\n",
    "        f.write(docs[j]+\" \")\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TJ]",
   "language": "python",
   "name": "conda-env-TJ-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
