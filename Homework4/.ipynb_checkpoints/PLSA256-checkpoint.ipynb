{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:22:19.311287Z",
     "start_time": "2019-11-17T07:22:18.354777Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from numba import jit\n",
    "from scipy.sparse import coo_matrix\n",
    "# import torch\n",
    "# device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:22:20.038121Z",
     "start_time": "2019-11-17T07:22:19.312265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of collection: 18461\n"
     ]
    }
   ],
   "source": [
    "text_file = open('./Collection.txt', \"r\")\n",
    "clcs = text_file.read().splitlines()\n",
    "clc_list = []\n",
    "for clc in clcs:\n",
    "    content = clc.split()\n",
    "    content = [x for x in content]\n",
    "    cnt_str = ' '.join(content)\n",
    "    clc_list.append(cnt_str)\n",
    "text_file.close()\n",
    "\n",
    "print('size of collection:',len(clc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:22:20.347316Z",
     "start_time": "2019-11-17T07:22:20.040116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of document: 2265\n"
     ]
    }
   ],
   "source": [
    "text_file = open('./doc_list.txt', \"r\")\n",
    "docs = text_file.read().splitlines()\n",
    "doc_list = []\n",
    "for doc in docs:\n",
    "    f = open('./Document/' + doc)\n",
    "    content = f.read().split()[5:]\n",
    "    content = [x for x in content if x != '-1']\n",
    "    cnt_str = ' '.join(content)\n",
    "    doc_list.append(cnt_str)\n",
    "text_file.close()\n",
    "\n",
    "print('size of document:', len(doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:22:20.483929Z",
     "start_time": "2019-11-17T07:22:20.349289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of query: 800\n"
     ]
    }
   ],
   "source": [
    "text_file = open('./query_list.txt', \"r\")\n",
    "queries = text_file.read().splitlines()\n",
    "qry_list = []\n",
    "for qry in queries:\n",
    "    f = open('./Query/' + qry)\n",
    "    content = f.read().split()\n",
    "    content = [x for x in content if x != '-1']\n",
    "    qry_list.append(content)\n",
    "text_file.close()\n",
    "\n",
    "print('size of query:', len(qry_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:22:20.598633Z",
     "start_time": "2019-11-17T07:22:20.484926Z"
    }
   },
   "outputs": [],
   "source": [
    "text_file = open('./BGLM.txt', \"r\")\n",
    "BGLM = text_file.read().splitlines()\n",
    "idf={}\n",
    "for line in BGLM:\n",
    "    (word,value)=line.split()\n",
    "    idf[word]=np.exp(float(value))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:22:24.230928Z",
     "start_time": "2019-11-17T07:22:20.599620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 35028\n"
     ]
    }
   ],
   "source": [
    "# build clc_tf, vocabulary\n",
    "vectorizer = CountVectorizer(token_pattern='[0-9]+', min_df = 1)\n",
    "clc_tf = vectorizer.fit_transform(clc_list+doc_list).tocoo()\n",
    "vocabulary = vectorizer.vocabulary_  # Mapping of {word -> col of doc_term}\n",
    "print('size of vocabulary:', len(vocabulary))\n",
    "\n",
    "# build doc_tf\n",
    "doc_tf = vectorizer.transform(doc_list).tocoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Pwt,Ptd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:22:24.236912Z",
     "start_time": "2019-11-17T07:22:24.232902Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_Pwt(T, V):\n",
    "    Pwt=np.random.rand(T, V)\n",
    "    Pwt /=  Pwt.sum(axis=1,keepdims=True)\n",
    "    return Pwt\n",
    "\n",
    "def init_Ptd(D, T):\n",
    "    Ptd=np.random.rand(D, T)\n",
    "    Ptd /=  Ptd.sum(axis=1,keepdims=True)\n",
    "    return Ptd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:22:24.297756Z",
     "start_time": "2019-11-17T07:22:24.238888Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def E_step(tf_data,tf_row,tf_col, Pwt, Ptd, T):\n",
    "    nnz=len(tf_data)\n",
    "    Ptwd = np.zeros((T, nnz))    \n",
    "    for ij in range(nnz):\n",
    "        j, i = tf_row[ij], tf_col[ij]\n",
    "        Ptwd_numerator = np.zeros(T)\n",
    "        Ptwd_denominator = 0\n",
    "        for k in range(T):\n",
    "            Ptwd_numerator[k] = Pwt[k][i] * Ptd[j][k]\n",
    "            Ptwd_denominator += Pwt[k][i] * Ptd[j][k]\n",
    "        for k in range(T):\n",
    "            if Ptwd_denominator!=0:\n",
    "                Ptwd[k][ij] = Ptwd_numerator[k] / Ptwd_denominator\n",
    "    return Ptwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:22:24.383531Z",
     "start_time": "2019-11-17T07:22:24.299725Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def M_step(tf_data,tf_row,tf_col, Ptwd, clc_len, V, C, T):\n",
    "\n",
    "    Ptd = np.zeros((C, T))\n",
    "    Pwt = np.zeros((T, V))\n",
    "    Pwt_denominator = np.zeros(T)\n",
    "    for ij in range(len(tf_data)):\n",
    "        j, i = tf_row[ij], tf_col[ij]\n",
    "        for k in range(T):            \n",
    "            Pwt[k][i] += tf_data[ij]*Ptwd[k][ij]\n",
    "            Pwt_denominator[k] += tf_data[ij]*Ptwd[k][ij]\n",
    "            Ptd[j][k] += tf_data[ij]*Ptwd[k][ij]\n",
    "    for k in range(T): \n",
    "        for i in range(V):\n",
    "            Pwt[k][i] = Pwt[k][i] / Pwt_denominator[k] \n",
    "    for k in range(T):\n",
    "        for j in range(C):\n",
    "            Ptd[j][k] = Ptd[j][k] / clc_len[j]\n",
    "    return Pwt, Ptd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M step (fold-in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:22:24.455624Z",
     "start_time": "2019-11-17T07:22:24.384498Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def M_step_fold_in(tf_data,tf_row,tf_col, Ptwd, doc_len, D, T):\n",
    "\n",
    "    Ptd_fdn = np.zeros((D, T))\n",
    "    for ij in range(len(tf_data)):\n",
    "        j, i = tf_row[ij], tf_col[ij]\n",
    "        for k in range(T):\n",
    "            Ptd_fdn[j][k] += tf_data[ij]*Ptwd[k][ij]  \n",
    "            \n",
    "    for k in range(T): \n",
    "        for j in range(D):\n",
    "            Ptd_fdn[j][k] = Ptd_fdn[j][k] / doc_len[j]  \n",
    "    return Ptd_fdn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:22:24.532305Z",
     "start_time": "2019-11-17T07:22:24.456626Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def likelihood(tf_data, tf_row, tf_col, Pwt, Ptd, T):\n",
    "    likelihood=0\n",
    "    for ij in range(len(tf_data)):\n",
    "        j,i = tf_row[ij], tf_col[ij]\n",
    "        sumation=0\n",
    "        for k in range(T):\n",
    "            sumation+=Pwt[k][i]*Ptd[j][k]\n",
    "        likelihood+=tf_data[ij]*np.log(sumation)\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:22:24.609541Z",
     "start_time": "2019-11-17T07:22:24.535299Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def similarity(alpha, beta, Pwd, PLSA):\n",
    "\n",
    "    sim = np.zeros((Q, D))\n",
    "    for q in range(Q):\n",
    "        for word in (qry_list[q]):\n",
    "                if word in vocabulary:\n",
    "                    i = vocabulary[word]\n",
    "                    A = alpha * Pwd[:,i]\n",
    "                    B = beta * PLSA[:,i]\n",
    "                    C = (1 - alpha - beta) * idf[word]\n",
    "                    sim[q,:] += np.log(A + B + C)\n",
    "        if q % 50 == 49:\n",
    "            print(\"query:\",q+1)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:22:27.747833Z",
     "start_time": "2019-11-17T07:22:24.611534Z"
    }
   },
   "outputs": [],
   "source": [
    "C = len(clc_list+doc_list)\n",
    "V = len(vocabulary)\n",
    "T = 256\n",
    "Pwt = init_Pwt(T, V)\n",
    "Ptd = init_Ptd(C, T)\n",
    "# Pwt = np.load(\"Pwt.npy\")\n",
    "# Ptd = np.load(\"Ptd.npy\")\n",
    "clc_data = clc_tf.data\n",
    "clc_row = clc_tf.row\n",
    "clc_col = clc_tf.col\n",
    "clc_len = clc_tf.toarray().sum(axis=1).reshape((C,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:52:32.838919Z",
     "start_time": "2019-11-17T07:22:27.749807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 \tlikelihood: -62569364.47501534\n",
      "iter: 50 \tlikelihood: -38364480.48140072\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3dd8ed189d35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"iter:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\tlikelihood:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclc_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclc_row\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclc_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPwt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPtd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mPtwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mE_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclc_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclc_row\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclc_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPwt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPtd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mPwt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPtd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclc_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclc_row\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclc_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPtwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclc_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m49\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_iter = 200\n",
    "print(\"iter:\", 0, \"\\tlikelihood:\", likelihood(clc_data,clc_row,clc_col,Pwt,Ptd,T))\n",
    "for count in range(200):\n",
    "    Ptwd = E_step(clc_data,clc_row,clc_col, Pwt, Ptd, T)\n",
    "    Pwt, Ptd = M_step(clc_data,clc_row,clc_col, Ptwd, clc_len, V, C, T)\n",
    "    if count%50==49:\n",
    "        np.save('Pwt', Pwt)\n",
    "        np.save('Ptd', Ptd)\n",
    "        print(\"iter:\", count+1, \"\\tlikelihood:\", likelihood(clc_data,clc_row,clc_col,Pwt,Ptd,T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fold-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:52:32.840899Z",
     "start_time": "2019-11-17T07:22:18.377Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D = len(doc_list)\n",
    "# Pwt = np.load(\"Pwt.npy\")\n",
    "Ptd_fdn = init_Ptd(D, T)\n",
    "# Ptd_fdn = np.load(\"Ptd_fdn.npy\")\n",
    "doc_data = doc_tf.data\n",
    "doc_row = doc_tf.row\n",
    "doc_col = doc_tf.col\n",
    "doc_len = doc_tf.toarray().sum(axis=1).reshape((D,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:52:32.841896Z",
     "start_time": "2019-11-17T07:22:18.379Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fold_in_iter = 200\n",
    "print(\"iter:\", 0, \"\\tlikelihood:\", likelihood(doc_data,doc_row,doc_col,Pwt,Ptd_fdn,T))\n",
    "for count in range(200):\n",
    "    Ptwd = E_step(doc_data,doc_row,doc_col, Pwt, Ptd_fdn, T)\n",
    "    Ptd_fdn = M_step_fold_in(doc_data,doc_row,doc_col, Ptwd, doc_len, D, T)\n",
    "    if count%50==49:\n",
    "        np.save('Ptd_fdn', Ptd_fdn)\n",
    "        print(\"iter:\", count+1, \"\\tlikelihood:\", likelihood(doc_data,doc_row,doc_col,Pwt,Ptd_fdn,T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:52:32.842895Z",
     "start_time": "2019-11-17T07:22:18.381Z"
    }
   },
   "outputs": [],
   "source": [
    "Q = len(qry_list)\n",
    "D = len(doc_list)\n",
    "# Ptd_fdn = np.load(\"Ptd_fdn.npy\")\n",
    "# Pwt = np.load(\"Pwt.npy\")\n",
    "Pwd = doc_tf.toarray() / doc_len.reshape((D,1))\n",
    "PLSA = np.dot(Ptd_fdn, Pwt)\n",
    "sim = similarity(0.1, 0.52, Pwd, PLSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:52:32.844889Z",
     "start_time": "2019-11-17T07:22:18.382Z"
    }
   },
   "outputs": [],
   "source": [
    "fname = \"./256_0.1_0.52.txt\"\n",
    "Rq_file = open(\"./Rq.txt\", 'w')\n",
    "f = open(fname, 'w')\n",
    "f.write(\"Query,RetrievedDocuments\\n\")  \n",
    "\n",
    "for q in range(len(qry_list)):\n",
    "    f.write(queries[q] + \",\")        \n",
    "    rank = np.argsort(-sim[q])\n",
    "    for j in rank[:50]:\n",
    "        Rq_file.write(str(j)+\" \")\n",
    "        f.write(docs[j]+\" \")\n",
    "    f.write(\"\\n\")\n",
    "    Rq_file.write(\"\\n\")\n",
    "f.close()\n",
    "Rq_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T11:26:04.541046Z",
     "start_time": "2019-11-15T11:26:04.531769Z"
    }
   },
   "source": [
    "# Read file Rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:17:26.264628Z",
     "start_time": "2019-11-17T07:17:26.245678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of Rq: (800, 50)\n"
     ]
    }
   ],
   "source": [
    "text_file = open('./Rq.txt', \"r\")\n",
    "Rqs = text_file.read().splitlines()\n",
    "Rq_list = []\n",
    "for Rq in Rqs:\n",
    "    content = Rq.split()\n",
    "    content = [int(j) for j in content]\n",
    "    Rq_list.append(content)\n",
    "text_file.close()\n",
    "\n",
    "print('size of Rq:',(len(Rq_list),len(Rq_list[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:18:07.036157Z",
     "start_time": "2019-11-17T07:18:07.032167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1701, 2240, 2254, 2260, 63, 1713, 1638, 1628, 971, 644]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rq_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T07:18:20.915505Z",
     "start_time": "2019-11-17T07:18:20.911515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1701 2240 2254 2260 63 1713 1638 1628 971 644 47 1508 355 1443 817 1700 271 1500 1712 1720 1702 259 816 332 331 506 233 151 330 2005 475 1398 1807 508 1335 970 264 1000 243 2166 751 1821 753 1338 477 1328 2239 692 1200 1520 '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rqs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T11:26:04.541046Z",
     "start_time": "2019-11-15T11:26:04.531769Z"
    }
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T06:12:23.572964Z",
     "start_time": "2019-11-17T06:12:21.093502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qry_tfidf.shape: (800, 35028)\n",
      "doc_tfidf.shape: (2265, 35028)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "qry_str_list=[]\n",
    "for qry in qry_list:\n",
    "    cnt_str = ' '.join(qry)\n",
    "    qry_str_list.append(cnt_str)\n",
    "qry_tf = vectorizer.transform(qry_str_list).tocoo() \n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "qry_tfidf = transformer.fit_transform(qry_tf.toarray()).toarray()\n",
    "doc_tfidf = transformer.fit_transform(doc_tf.toarray()).toarray()\n",
    "print(\"qry_tfidf.shape:\",qry_tfidf.shape)\n",
    "print(\"doc_tfidf.shape:\",doc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Rocchio Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T06:29:16.999086Z",
     "start_time": "2019-11-17T06:29:16.994557Z"
    }
   },
   "outputs": [],
   "source": [
    "def feedback_qry_tfidf(qry_tfidf,doc_tfidf,Rq_list,rev_num,alpha,beta,Q,V):\n",
    "    feedback_qry_tfidf=np.zeros((Q, V))\n",
    "    for q in range(Q):\n",
    "        rev_doc_tfidf_sum=np.zeros(V)\n",
    "        for r in range(rev_num):\n",
    "            j = Rq_list[q][r]\n",
    "            rev_doc_tfidf_sum += doc_tfidf[j]\n",
    "        feedback_qry_tfidf[q] = alpha*qry_tfidf[q] + beta*rev_doc_tfidf_sum/rev_num\n",
    "    return feedback_qry_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T06:43:51.784259Z",
     "start_time": "2019-11-17T06:43:51.229725Z"
    }
   },
   "outputs": [],
   "source": [
    "rev_num=10\n",
    "alpha,beta=(0.8,0.2)\n",
    "new_qry_tfidf=feedback_qry_tfidf(qry_tfidf,doc_tfidf,Rq_list,rev_num,alpha,beta,Q,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T06:43:52.872902Z",
     "start_time": "2019-11-17T06:43:52.868293Z"
    }
   },
   "outputs": [],
   "source": [
    "def cos_sim(v1,v2):\n",
    "    return np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRF result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T06:43:55.653368Z",
     "start_time": "2019-11-17T06:43:54.100561Z"
    }
   },
   "outputs": [],
   "source": [
    "fname = \"./\"+str(rev_num)+\"_\"+str(alpha)+\"_\"+str(beta)+\".txt\"\n",
    "f = open(fname, 'w')\n",
    "f.write(\"Query,RetrievedDocuments\\n\")  \n",
    "\n",
    "for q in range(len(qry_list)):\n",
    "    f.write(queries[q] + \",\")   \n",
    "    \n",
    "    for j in range(50):         \n",
    "        if j==0:\n",
    "            sim=cos_sim(new_qry_tfidf[q],doc_tfidf[j])\n",
    "        else:\n",
    "            sim=np.append(sim,cos_sim(new_qry_tfidf[q],doc_tfidf[j]))\n",
    "        \n",
    "    rank = np.argsort(-sim)\n",
    "    for j in rank:\n",
    "        f.write(docs[j]+\" \")\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
