{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T02:47:02.198978Z",
     "start_time": "2019-11-18T02:47:01.374274Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from numba import jit\n",
    "from scipy.sparse import coo_matrix\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T02:47:02.862879Z",
     "start_time": "2019-11-18T02:47:02.200781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of collection: 18461\n"
     ]
    }
   ],
   "source": [
    "text_file = open('./Collection.txt', \"r\")\n",
    "clcs = text_file.read().splitlines()\n",
    "clc_list = []\n",
    "for clc in clcs:\n",
    "    content = clc.split()\n",
    "    content = [x for x in content]\n",
    "    cnt_str = ' '.join(content)\n",
    "    clc_list.append(cnt_str)\n",
    "text_file.close()\n",
    "\n",
    "print('size of collection:',len(clc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T02:47:03.322644Z",
     "start_time": "2019-11-18T02:47:02.864699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of document: 2265\n"
     ]
    }
   ],
   "source": [
    "text_file = open('./doc_list.txt', \"r\")\n",
    "docs = text_file.read().splitlines()\n",
    "doc_list = []\n",
    "for doc in docs:\n",
    "    f = open('./Document/' + doc)\n",
    "    content = f.read().split()[5:]\n",
    "    content = [x for x in content if x != '-1']\n",
    "    cnt_str = ' '.join(content)\n",
    "    doc_list.append(cnt_str)\n",
    "text_file.close()\n",
    "\n",
    "print('size of document:', len(doc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T02:47:03.509887Z",
     "start_time": "2019-11-18T02:47:03.324500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of query: 800\n"
     ]
    }
   ],
   "source": [
    "text_file = open('./query_list.txt', \"r\")\n",
    "queries = text_file.read().splitlines()\n",
    "qry_list = []\n",
    "for qry in queries:\n",
    "    f = open('./Query/' + qry)\n",
    "    content = f.read().split()\n",
    "    content = [x for x in content if x != '-1']\n",
    "    qry_list.append(content)\n",
    "text_file.close()\n",
    "\n",
    "print('size of query:', len(qry_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T02:47:03.641621Z",
     "start_time": "2019-11-18T02:47:03.511681Z"
    }
   },
   "outputs": [],
   "source": [
    "text_file = open('./BGLM.txt', \"r\")\n",
    "BGLM = text_file.read().splitlines()\n",
    "idf={}\n",
    "for line in BGLM:\n",
    "    (word,value)=line.split()\n",
    "    idf[word]=np.exp(float(value))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T02:47:07.814043Z",
     "start_time": "2019-11-18T02:47:03.643414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 35028\n"
     ]
    }
   ],
   "source": [
    "# build clc_tf, vocabulary\n",
    "vectorizer = CountVectorizer(token_pattern='[0-9]+', min_df = 1)\n",
    "clc_tf = vectorizer.fit_transform(clc_list+doc_list).tocoo()\n",
    "vocabulary = vectorizer.vocabulary_  # Mapping of {word -> col of doc_term}\n",
    "print('size of vocabulary:', len(vocabulary))\n",
    "\n",
    "# build doc_tf\n",
    "doc_tf = vectorizer.transform(doc_list).tocoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Pwt,Ptd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T02:47:07.819033Z",
     "start_time": "2019-11-18T02:47:07.815851Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_Pwt(T, V):\n",
    "    Pwt=np.random.rand(T, V)\n",
    "    Pwt /=  Pwt.sum(axis=1,keepdims=True)\n",
    "    return Pwt\n",
    "\n",
    "def init_Ptd(D, T):\n",
    "    Ptd=np.random.rand(D, T)\n",
    "    Ptd /=  Ptd.sum(axis=1,keepdims=True)\n",
    "    return Ptd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T02:47:07.851954Z",
     "start_time": "2019-11-18T02:47:07.820885Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def E_step(tf_data,tf_row,tf_col, Pwt, Ptd, T):\n",
    "    nnz=len(tf_data)\n",
    "    Ptwd = np.zeros((T, nnz))    \n",
    "    for ij in range(nnz):\n",
    "        j, i = tf_row[ij], tf_col[ij]\n",
    "        Ptwd_numerator = np.zeros(T)\n",
    "        Ptwd_denominator = 0\n",
    "        for k in range(T):\n",
    "            Ptwd_numerator[k] = Pwt[k][i] * Ptd[j][k]\n",
    "            Ptwd_denominator += Pwt[k][i] * Ptd[j][k]\n",
    "        for k in range(T):\n",
    "            if Ptwd_denominator!=0:\n",
    "                Ptwd[k][ij] = Ptwd_numerator[k] / Ptwd_denominator\n",
    "    return Ptwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T02:47:07.858810Z",
     "start_time": "2019-11-18T02:47:07.853789Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def M_step(tf_data,tf_row,tf_col, Ptwd, clc_len, V, C, T):\n",
    "\n",
    "    Ptd = np.zeros((C, T))\n",
    "    Pwt = np.zeros((T, V))\n",
    "    Pwt_denominator = np.zeros(T)\n",
    "    for ij in range(len(tf_data)):\n",
    "        j, i = tf_row[ij], tf_col[ij]\n",
    "        for k in range(T):            \n",
    "            Pwt[k][i] += tf_data[ij]*Ptwd[k][ij]\n",
    "            Pwt_denominator[k] += tf_data[ij]*Ptwd[k][ij]\n",
    "            Ptd[j][k] += tf_data[ij]*Ptwd[k][ij]\n",
    "    for k in range(T): \n",
    "        for i in range(V):\n",
    "            Pwt[k][i] = Pwt[k][i] / Pwt_denominator[k] \n",
    "    for k in range(T):\n",
    "        for j in range(C):\n",
    "            Ptd[j][k] = Ptd[j][k] / clc_len[j]\n",
    "    return Pwt, Ptd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M step (fold-in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T02:47:07.864859Z",
     "start_time": "2019-11-18T02:47:07.860510Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def M_step_fold_in(tf_data,tf_row,tf_col, Ptwd, doc_len, D, T):\n",
    "\n",
    "    Ptd_fdn = np.zeros((D, T))\n",
    "    for ij in range(len(tf_data)):\n",
    "        j, i = tf_row[ij], tf_col[ij]\n",
    "        for k in range(T):\n",
    "            Ptd_fdn[j][k] += tf_data[ij]*Ptwd[k][ij]  \n",
    "            \n",
    "    for k in range(T): \n",
    "        for j in range(D):\n",
    "            Ptd_fdn[j][k] = Ptd_fdn[j][k] / doc_len[j]  \n",
    "    return Ptd_fdn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T02:47:07.870438Z",
     "start_time": "2019-11-18T02:47:07.866769Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def likelihood(tf_data, tf_row, tf_col, Pwt, Ptd, T):\n",
    "    likelihood=0\n",
    "    for ij in range(len(tf_data)):\n",
    "        j,i = tf_row[ij], tf_col[ij]\n",
    "        sumation=0\n",
    "        for k in range(T):\n",
    "            sumation+=Pwt[k][i]*Ptd[j][k]\n",
    "        likelihood+=tf_data[ij]*np.log(sumation)\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T02:47:07.876416Z",
     "start_time": "2019-11-18T02:47:07.872337Z"
    }
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def similarity(alpha, beta, Pwd, PLSA):\n",
    "\n",
    "    sim = np.zeros((Q, D))\n",
    "    for q in range(Q):\n",
    "        for word in (qry_list[q]):\n",
    "                if word in vocabulary:\n",
    "                    i = vocabulary[word]\n",
    "                    A = alpha * Pwd[:,i]\n",
    "                    B = beta * PLSA[:,i]\n",
    "                    C = (1 - alpha - beta) * idf[word]\n",
    "                    sim[q,:] += np.log(A + B + C)\n",
    "        if q % 100 == 99:\n",
    "            print(\"query:\",q+1)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T02:47:10.768962Z",
     "start_time": "2019-11-18T02:47:07.878150Z"
    }
   },
   "outputs": [],
   "source": [
    "C = len(clc_list+doc_list)\n",
    "V = len(vocabulary)\n",
    "T = 100\n",
    "Pwt = init_Pwt(T, V)\n",
    "Ptd = init_Ptd(C, T)\n",
    "# Pwt = np.load(\"Pwt\"+str(T)+\".npy\")\n",
    "# Ptd = np.load(\"Ptd\"+str(T)+\".npy\")\n",
    "clc_data = clc_tf.data\n",
    "clc_row = clc_tf.row\n",
    "clc_col = clc_tf.col\n",
    "clc_len = clc_tf.toarray().sum(axis=1).reshape((C,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:42:19.379750Z",
     "start_time": "2019-11-18T02:47:22.358058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 \tlikelihood: -62517719.20683333\n",
      "iter: 50 \tlikelihood: -39785034.73117615\n",
      "iter: 100 \tlikelihood: -39573402.144252844\n",
      "iter: 150 \tlikelihood: -39516200.957199074\n",
      "iter: 200 \tlikelihood: -39485916.93635931\n",
      "iter: 250 \tlikelihood: -39466892.58322004\n",
      "iter: 300 \tlikelihood: -39453721.56325345\n"
     ]
    }
   ],
   "source": [
    "train_iter = 0\n",
    "print(\"iter:\", train_iter, \"\\tlikelihood:\", likelihood(clc_data,clc_row,clc_col,Pwt,Ptd,T))\n",
    "for count in range(train_iter,train_iter+300):\n",
    "    Ptwd = E_step(clc_data,clc_row,clc_col, Pwt, Ptd, T)\n",
    "    Pwt, Ptd = M_step(clc_data,clc_row,clc_col, Ptwd, clc_len, V, C, T)\n",
    "    if count%50==49:\n",
    "        np.save('Pwt'+str(T), Pwt)\n",
    "        np.save('Ptd'+str(T), Ptd)\n",
    "        print(\"iter:\", count+1, \"\\tlikelihood:\", likelihood(clc_data,clc_row,clc_col,Pwt,Ptd,T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fold-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:42:19.685131Z",
     "start_time": "2019-11-18T03:42:19.381663Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D = len(doc_list)\n",
    "# Pwt = np.load(\"Pwt\"+str(T)+\".npy\")\n",
    "Ptd_fdn = init_Ptd(D, T)\n",
    "# Ptd_fdn = np.load(\"Ptd_fdn\"+str(T)+\".npy\")\n",
    "doc_data = doc_tf.data\n",
    "doc_row = doc_tf.row\n",
    "doc_col = doc_tf.col\n",
    "doc_len = doc_tf.toarray().sum(axis=1).reshape((D,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:45:34.702521Z",
     "start_time": "2019-11-18T03:42:19.686907Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0 \tlikelihood: -3016980.9023977485\n",
      "iter: 100 \tlikelihood: -2563299.6287861774\n",
      "iter: 200 \tlikelihood: -2563296.113640341\n",
      "iter: 300 \tlikelihood: -2563295.690242161\n",
      "iter: 400 \tlikelihood: -2563295.570108653\n"
     ]
    }
   ],
   "source": [
    "fold_in_iter = 0\n",
    "print(\"iter:\", fold_in_iter, \"\\tlikelihood:\", likelihood(doc_data,doc_row,doc_col,Pwt,Ptd_fdn,T))\n",
    "for count in range(fold_in_iter,fold_in_iter+400):\n",
    "    Ptwd = E_step(doc_data,doc_row,doc_col, Pwt, Ptd_fdn, T)\n",
    "    Ptd_fdn = M_step_fold_in(doc_data,doc_row,doc_col, Ptwd, doc_len, D, T)\n",
    "    if count%100==99:\n",
    "        np.save('Ptd_fdn'+str(T), Ptd_fdn)\n",
    "        print(\"iter:\", count+1, \"\\tlikelihood:\", likelihood(doc_data,doc_row,doc_col,Pwt,Ptd_fdn,T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance degree"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T10:37:04.366704Z",
     "start_time": "2019-11-17T10:37:04.363072Z"
    }
   },
   "source": [
    "T      train    fold-in    score\n",
    "100    300      400        0.61198\n",
    "128    200      200        0.60549\n",
    "256    200      200        0.56181\n",
    "256    200      400        0.56195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:46:31.464722Z",
     "start_time": "2019-11-18T03:45:34.704393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: 50\n",
      "query: 100\n",
      "query: 150\n",
      "query: 200\n",
      "query: 250\n",
      "query: 300\n",
      "query: 350\n",
      "query: 400\n",
      "query: 450\n",
      "query: 500\n",
      "query: 550\n",
      "query: 600\n",
      "query: 650\n",
      "query: 700\n",
      "query: 750\n",
      "query: 800\n"
     ]
    }
   ],
   "source": [
    "Q = len(qry_list)\n",
    "D = len(doc_list)\n",
    "# Ptd_fdn = np.load(\"Ptd_fdn\"+str(T)+\".npy\")\n",
    "# Pwt = np.load(\"Pwt\"+str(T)+\".npy\")\n",
    "Pwd = doc_tf.toarray() / doc_len.reshape((D,1))\n",
    "PLSA = np.dot(Ptd_fdn, Pwt)\n",
    "np.save('Pwd'+str(T),Pwd)\n",
    "np.save('PLSA'+str(T),PLSA)\n",
    "sim = similarity(0.1, 0.52, Pwd, PLSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:46:31.695019Z",
     "start_time": "2019-11-18T03:46:31.466604Z"
    }
   },
   "outputs": [],
   "source": [
    "fname = \"./\"+str(T)+\"_0.1_0.52.txt\"\n",
    "Rq_file = open(\"./Rq\"+str(T)+\".txt\", 'w')\n",
    "f = open(fname, 'w')\n",
    "f.write(\"Query,RetrievedDocuments\\n\")  \n",
    "\n",
    "for q in range(len(qry_list)):\n",
    "    f.write(queries[q] + \",\")        \n",
    "    rank = np.argsort(-sim[q])\n",
    "    for j in rank[:50]:\n",
    "        Rq_file.write(str(j)+\" \")\n",
    "        f.write(docs[j]+\" \")\n",
    "    f.write(\"\\n\")\n",
    "    Rq_file.write(\"\\n\")\n",
    "f.close()\n",
    "Rq_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TJ]",
   "language": "python",
   "name": "conda-env-TJ-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
